{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to compare a few approaches to solving the multi-arm bandit problem.\n",
    "# To me, this seems like a stupid application of reinforcement learning. Standard\n",
    "# statistical modelling methods should outperform RL on a per sample basis, i.e.,\n",
    "# for a fixed sized training set, statistical modelling will give larger average\n",
    "# payouts. The experimental setup is as follows:\n",
    "#\n",
    "# Consider an n-arm bandit whose payouts are given by:\n",
    "#   i. n Gaussian random variables\n",
    "#   ii. n uniform random variables\n",
    "#   iii. A mixture of i. and ii.\n",
    "#\n",
    "# ... TBD\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BanditGaussian():\n",
    "    \n",
    "    def __init__(self, n=4, mu=[0.1, 0.2, 0.5, 0.9], sigma=[10, 1, 8, 4]):\n",
    "        \n",
    "        self.n = n\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        assert n == len(mu) == len(sigma)\n",
    "        \n",
    "    def draw(self, lever, N=1):\n",
    "        \n",
    "        mu, sigma = self.mu[lever], self.sigma[lever]\n",
    "        return mu + sigma * np.random.randn(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BanditBernouli():\n",
    "    \n",
    "    def __init__(self, n=4, ps=[0.1, 0.2, 0.5, 0.9]):\n",
    "        \n",
    "        self.n = n\n",
    "        self.ps = ps\n",
    "        assert 0 <= min(ps) <= max(ps) <= 1\n",
    "        \n",
    "    def draw(self, lever, N=1):\n",
    "        \n",
    "        p = self.ps[lever]\n",
    "        return np.random.rand(N) > p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def Agent():\n",
    "    \n",
    "    def __init__(self, bandit):\n",
    "        \n",
    "        self.bandit = bandit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Super naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = BanditGaussian()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.10242290394681143 10.001510741608246 0.09997567231516227 954000\n",
      "1 0.1890801221704046 0.9836282578851897 0.09497832256840566 3000\n",
      "2 0.46654899071953837 8.000647610829754 0.09739732766762933 31000\n",
      "3 0.8816140314581021 3.9928211746221645 0.08268766706627168 3000\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# pull each lever until we have a confident estimate of the mean reward\n",
    "# this assumes an underlying Gaussian distribution. but that doesn't mean\n",
    "# it wouldn't work for other distributions.\n",
    "#\n",
    "\n",
    "N = 1000\n",
    "\n",
    "for lever in [0, 1, 2, 3]:\n",
    "    mu = 1.\n",
    "    dmu = 10000 * mu\n",
    "    rewards = np.array([])\n",
    "    \n",
    "    while abs(dmu / mu) > 0.1:\n",
    "        rewards = np.hstack((rewards, b.draw(lever, N=N)))\n",
    "        mu = rewards.mean()\n",
    "        dmu = rewards.std() / np.sqrt(len(rewards))\n",
    "    \n",
    "    print lever, rewards.mean(), rewards.std(), dmu / mu, len(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = BanditBernouli()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.89 0.31288975694324034 0.03515615246553262 100\n",
      "1 0.81 0.39230090491866065 0.04843221048378526 100\n",
      "2 0.47 0.49909918853871116 0.07508860014902678 200\n",
      "3 0.10777777777777778 0.3100995459446237 0.09590707606534754 900\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# pull each lever until we have a confident estimate of the mean reward\n",
    "# this assumes an underlying Gaussian distribution. but that doesn't mean\n",
    "# it wouldn't work for other distributions.\n",
    "#\n",
    "\n",
    "N = 100\n",
    "\n",
    "for lever in [0, 1, 2, 3]:\n",
    "    mu = 1.\n",
    "    dmu = 10000 * mu\n",
    "    rewards = np.array([])\n",
    "    \n",
    "    while abs(dmu / mu) > 0.1:\n",
    "        rewards = np.hstack((rewards, b.draw(lever, N=N)))\n",
    "        mu = rewards.mean()\n",
    "        dmu = rewards.std() / np.sqrt(len(rewards))\n",
    "    \n",
    "    print lever, rewards.mean(), rewards.std(), dmu / mu, len(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = BanditBernouli()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# These two lines established the feed-forward part of the network.\n",
    "# This does the actual choosing.\n",
    "weights = tf.Variable(tf.ones([b.n]))\n",
    "chosen_action = tf.argmax(weights,0)\n",
    "\n",
    "# The next six lines establish the training proceedure. We feed the \n",
    "# reward and chosen action into the network to compute the loss, and \n",
    "# use it to update the network.\n",
    "reward_holder = tf.placeholder(shape=[1],dtype=tf.float32)\n",
    "action_holder = tf.placeholder(shape=[1],dtype=tf.int32)\n",
    "responsible_weight = tf.slice(weights, action_holder,[1])\n",
    "loss = -(tf.log(responsible_weight)*reward_holder)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "update = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running reward for the 4 bandits: [1. 0. 0. 0.]\n",
      "Running reward for the 4 bandits: [42.  0.  1.  0.]\n",
      "Running reward for the 4 bandits: [79.  0.  2.  0.]\n",
      "Running reward for the 4 bandits: [122.   2.   3.   0.]\n",
      "Running reward for the 4 bandits: [163.   2.   3.   0.]\n",
      "Running reward for the 4 bandits: [206.   3.   3.   0.]\n",
      "Running reward for the 4 bandits: [250.   6.   3.   0.]\n",
      "Running reward for the 4 bandits: [293.   8.   3.   2.]\n",
      "Running reward for the 4 bandits: [327.   8.   3.   2.]\n",
      "Running reward for the 4 bandits: [372.   8.   3.   3.]\n",
      "Running reward for the 4 bandits: [413.   8.   3.   3.]\n",
      "Running reward for the 4 bandits: [455.  10.   3.   3.]\n",
      "Running reward for the 4 bandits: [494.  12.   4.   4.]\n",
      "Running reward for the 4 bandits: [534.  12.   4.   4.]\n",
      "Running reward for the 4 bandits: [576.  12.   5.   4.]\n",
      "Running reward for the 4 bandits: [617.  14.   7.   4.]\n",
      "Running reward for the 4 bandits: [663.  15.   7.   4.]\n",
      "Running reward for the 4 bandits: [706.  17.   8.   4.]\n",
      "Running reward for the 4 bandits: [748.  18.   8.   5.]\n",
      "Running reward for the 4 bandits: [791.  19.  10.   5.]\n",
      "The agent thinks bandit 1 is the most promising....\n",
      "...and it was right!\n"
     ]
    }
   ],
   "source": [
    "total_episodes = 1000 #Set total number of episodes to train agent on.\n",
    "total_reward = np.zeros(b.n) #Set scoreboard for bandits to 0.\n",
    "e = 0.1 #Set the chance of taking a random action.\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Launch the tensorflow graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    i = 0\n",
    "    while i < total_episodes:\n",
    "        \n",
    "        #Choose either a random action or one from our network.\n",
    "        if np.random.rand(1) < e:\n",
    "            action = np.random.randint(b.n)\n",
    "        else:\n",
    "            action = sess.run(chosen_action)\n",
    "        \n",
    "        reward = b.draw(action)[0] #Get our reward from picking one of the bandits.\n",
    "        \n",
    "        #Update the network.\n",
    "        _,resp,ww = sess.run([update,responsible_weight,weights], feed_dict={reward_holder:[reward],action_holder:[action]})\n",
    "        \n",
    "        #Update our running tally of scores.\n",
    "        total_reward[action] += reward\n",
    "        if i % 50 == 0:\n",
    "            print \"Running reward for the \" + str(b.n) + \" bandits: \" + str(total_reward)\n",
    "        i+=1\n",
    "print \"The agent thinks bandit \" + str(np.argmax(ww)+1) + \" is the most promising....\"\n",
    "if np.argmax(ww) == np.argmax(-np.array(b.ps)):\n",
    "    print \"...and it was right!\"\n",
    "else:\n",
    "    print \"...and it was wrong!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
